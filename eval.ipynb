{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ecec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\") # make a .env for this and put your access token as HF_TOKEN=whateverYourAccessTokenIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ec586",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "#model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#print(\"GPU available \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53734723",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_token\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60060df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, tokenizer, messages, max_new_tokens=5, verbose=False):\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    if verbose: print(\"\\n###input_text:###\\n\", input_text)\n",
    "\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    if verbose: print(\"\\n###input_ids:###\\n\", input_ids)\n",
    "\n",
    "    terminators = [\n",
    "      tokenizer.eos_token_id,\n",
    "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Decode the output and return the response without special tokens\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    if verbose: print(\"\\n###response:###\\n\", response)\n",
    "    \n",
    "    start_marker = \"python\\n\"\n",
    "    end_marker = \"```\"\n",
    "    assistant_response = response.split(start_marker)[1].split(end_marker)[0] # grab just the code snippet\n",
    "    #assistant_response = response.split(\"\\n\")[-1].strip()\n",
    "    #assistant_response = response\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597b2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"lc_hard.json\", lines=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lc_prompt(desc, skel):\n",
    "    prompt = (\n",
    "        \"Your task is to complete the following problem in Python. You are provided with a skeleton code to complete and a description. Attempt to avoid importing modules as much as you can. Output your completed version of the code. \"\n",
    "        f\"Description: {desc}\"\n",
    "        \"Below is the starting point for your code. \\n\"\n",
    "        f\"{skel}\"\n",
    "    )\n",
    "\n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56429c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.copy()\n",
    "dataset[\"prompt\"] = dataset.apply(lambda x: apply_lc_prompt(x[\"desc\"], x[\"ref\"]), axis=1)\n",
    "print(dataset.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score\n",
    "\n",
    "\n",
    "def eval_bleu(model, tokenizer, dataset, max_new_tokens=1000):\n",
    "    outputs = []\n",
    "\n",
    "    for row in tqdm(dataset.to_dict(orient=\"records\")):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"user\", \"content\": row[\"prompt\"]},\n",
    "        ]\n",
    "\n",
    "        output = run_model(model=model, tokenizer=tokenizer, messages=messages, max_new_tokens=max_new_tokens)\n",
    "\n",
    "        outputs.append(output)\n",
    "    \n",
    "    r, h = [], []\n",
    "    for idx, row in tqdm(enumerate(dataset.to_dict(orient=\"records\"))):\n",
    "        refs_in_dataset = row[\"ref\"]\n",
    "        references = []\n",
    "        for real_code_solution in refs_in_dataset:\n",
    "            references.append(real_code_solution.split())\n",
    "        hypothesis = outputs[idx].split()\n",
    "        \n",
    "        r.append(references)\n",
    "        h.append(hypothesis)\n",
    "    \n",
    "    bleu_score = nltk.translate.bleu_score.corpus_bleu(r, h, weights=(1,0,0,0))\n",
    "    return bleu_score, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b639203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.copy()\n",
    "bleu_score, outputs = eval_bleu(model, tokenizer, df)\n",
    "print(f\"Bleu: {bleu_score}\")\n",
    "df[\"output\"] = outputs\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListNode:\n",
    "    def __init__(self, val=0, next=None):\n",
    "        self.val = val\n",
    "        self.next = next\n",
    "\n",
    "import threading\n",
    "from typing import List, Optional\n",
    "\n",
    "# Function to execute a callable with a timeout\n",
    "def run_with_timeout(func, args, result_holder, timeout=5):\n",
    "    def wrapper():\n",
    "        try:\n",
    "            result_holder['result'] = func(*args)\n",
    "        except Exception as e:\n",
    "            result_holder['result'] = e\n",
    "    \n",
    "    # Create a thread to execute the function\n",
    "    thread = threading.Thread(target=wrapper)\n",
    "    thread.start()\n",
    "    thread.join(timeout)  # Wait for the specified timeout\n",
    "    \n",
    "    if thread.is_alive():\n",
    "        thread._stop()  # Forcefully stop the thread (not safe but works for this context)\n",
    "        result_holder['result'] = 'Timeout'\n",
    "        \n",
    "    return result_holder['result']\n",
    "\n",
    "def eval_test_case(code, test_inputs, expected_outputs, function_name):\n",
    "    try:\n",
    "        # Define the namespace and execute the code\n",
    "        namespace = {\n",
    "            'List': List,\n",
    "            'ListNode': ListNode,\n",
    "            'Optional': Optional\n",
    "        }\n",
    "        exec(code, namespace)\n",
    "        Solution = namespace.get(\"Solution\")\n",
    "        solution_instance = Solution()\n",
    "\n",
    "        # Get the function to test\n",
    "        func = getattr(solution_instance, function_name, None)\n",
    "        if not callable(func):\n",
    "            raise ValueError(f\"Function '{function_name}' is not defined or callable\")\n",
    "        \n",
    "        passed = 0\n",
    "        total = len(test_inputs)\n",
    "\n",
    "        for test_input, expected_output in zip(test_inputs, expected_outputs):\n",
    "            try:\n",
    "                # Create a result holder to capture the result of the function\n",
    "                result_holder = {'result': None}\n",
    "                \n",
    "                # Call the function with a timeout (5 seconds)\n",
    "                result = run_with_timeout(func, test_input, result_holder, timeout=5)\n",
    "                \n",
    "                # Check if the result is as expected\n",
    "                if result == expected_output:\n",
    "                    print(f\"Test with input {test_input} passed. Expected {expected_output}, got {result}\")\n",
    "                    passed += 1\n",
    "                else:\n",
    "                    # Optional: print if the result is incorrect\n",
    "                    print(f\"Test with input {test_input} failed. Expected {expected_output}, got {result}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Test with input {test_input} on {function_name} failed due to error: {e}\")\n",
    "        \n",
    "        return passed / total if total > 0 else 0.0\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during code execution: {e}\")\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_blocks = outputs\n",
    "\n",
    "total_pass_rate = 0\n",
    "\n",
    "num_iter = 0\n",
    "\n",
    "first_20_pass_rate = 0\n",
    "\n",
    "pass_rate_per_prob = {}\n",
    "\n",
    "for idx, code_block in enumerate(tqdm(code_blocks, desc=\"Evaluating Code Blocks\")):\n",
    "    if num_iter == 21:\n",
    "        first_20_pass_rate = total_pass_rate\n",
    "\n",
    "    test_dict = df.iloc[idx].to_dict()[\"test\"]\n",
    "\n",
    "    test_inputs = test_dict[\"input\"]\n",
    "    expected_outputs = test_dict[\"output\"]\n",
    "    function_name = df.iloc[idx].to_dict()[\"func\"]\n",
    "    \n",
    "    pass_rate = eval_test_case(code_block, test_inputs, expected_outputs, function_name)\n",
    "    total_pass_rate += pass_rate\n",
    "    pass_rate_per_prob[idx] = pass_rate\n",
    "\n",
    "    num_iter += 1\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "avg_pass_rate = total_pass_rate/len(code_blocks)\n",
    "print(\"Average Pass Rate: \", avg_pass_rate)\n",
    "print(\"First 20 Pass Rate: \", first_20_pass_rate/20)\n",
    "print(\"Last 30 Pass Rate: \", (total_pass_rate - first_20_pass_rate)/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07566f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pass Rate Per Problem\", pass_rate_per_prob)\n",
    "print(\"Average Pass Rate: \", avg_pass_rate)\n",
    "print(\"First 20 Pass Rate: \", first_20_pass_rate/20)\n",
    "print(\"Last 30 Pass Rate: \", (total_pass_rate - first_20_pass_rate)/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad069be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Folder to save the output files\n",
    "folder_name = \"generated_outputs_3b\"\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Loop through the list and create the files\n",
    "for idx, content in enumerate(outputs):\n",
    "    file_name = f\"lc{idx + 1}.txt\"  # Construct the file name\n",
    "    file_path = os.path.join(folder_name, file_name)  # Full file path\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)  # Write the content to the file\n",
    "\n",
    "print(f\"Files have been created in the '{folder_name}' folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
